# 修复代码生成提示词（实例ID：scikit-learn__scikit-learn-13447）
## 代码仓库
scikit-learn/scikit-learn

## 原始问题描述
label_ranking_average_precision_score: sample_weighting isn't applied to items with zero true labels
#### Description
label_ranking_average_precision_score offers a sample_weighting argument to allow nonuniform contribution of individual samples to the reported metric.  Separately, individual samples whose labels are the same for all classes (all true or all false) are treated as a special case (precision == 1, [line 732](https://github.com/scikit-learn/scikit-learn/blob/7b136e9/sklearn/metrics/ranking.py#L732)). However, this special case bypasses the application of sample_weight ([line 740](https://github.com/scikit-learn/scikit-learn/blob/7b136e9/sklearn/metrics/ranking.py#L740)).  So, in the case where there is both non-default sample_weighting and samples with, for instance, zero labels, the reported metric is wrong.

<!-- Example: Joblib Error thrown when calling fit on LatentDirichletAllocation with evaluate_every > 0-->

#### Steps/Code to Reproduce
See example in [this colab](https://colab.research.google.com/drive/19P-6UgIMZSUgBcLyR7jm9oELacrYNJE7)

```
import numpy as np
import sklearn.metrics

# Per sample APs are 0.5, 0.75, and 1.0 (default for zero labels).
truth = np.array([[1, 0, 0, 0], [1, 0, 0, 1], [0, 0, 0, 0]], dtype=np.bool)
scores = np.array([[0.3, 0.4, 0.2, 0.1], [0.1, 0.2, 0.3, 0.4], [0.4, 0.3, 0.2, 0.1]])
print(sklearn.metrics.label_ranking_average_precision_score(
    truth, scores, sample_weight=[1.0, 1.0, 0.0]))
```

#### Expected Results
Average of AP of first and second samples = 0.625

#### Actual Results
Sum of AP of all three samples, divided by sum of weighting vector = 2.25/2 = 1.125

#### Versions
System:
    python: 3.6.7 (default, Oct 22 2018, 11:32:17)  [GCC 8.2.0]
executable: /usr/bin/python3
   machine: Linux-4.14.79+-x86_64-with-Ubuntu-18.04-bionic

BLAS:
    macros: SCIPY_MKL_H=None, HAVE_CBLAS=None
  lib_dirs: /usr/local/lib
cblas_libs: mkl_rt, pthread

Python deps:
       pip: 19.0.3
setuptools: 40.8.0
   sklearn: 0.20.3
     numpy: 1.14.6
     scipy: 1.1.0
    Cython: 0.29.6
    pandas: 0.22.0

<!-- Thanks for contributing! -->



## 参考黄金补丁（正确的修复方案）
diff --git a/sklearn/metrics/tests/test_ranking.py b/sklearn/metrics/tests/test_ranking.py
--- a/sklearn/metrics/tests/test_ranking.py
+++ b/sklearn/metrics/tests/test_ranking.py
@@ -952,6 +952,25 @@ def test_alternative_lrap_implementation(n_samples, n_classes, random_state):
                n_classes, n_samples, random_state)
 
 
+def test_lrap_sample_weighting_zero_labels():
+    # Degenerate sample labeling (e.g., zero labels for a sample) is a valid
+    # special case for lrap (the sample is considered to achieve perfect
+    # precision), but this case is not tested in test_common.
+    # For these test samples, the APs are 0.5, 0.75, and 1.0 (default for zero
+    # labels).
+    y_true = np.array([[1, 0, 0, 0], [1, 0, 0, 1], [0, 0, 0, 0]],
+                      dtype=np.bool)
+    y_score = np.array([[0.3, 0.4, 0.2, 0.1], [0.1, 0.2, 0.3, 0.4],
+                        [0.4, 0.3, 0.2, 0.1]])
+    samplewise_lraps = np.array([0.5, 0.75, 1.0])
+    sample_weight = np.array([1.0, 1.0, 0.0])
+
+    assert_almost_equal(
+        label_ranking_average_precision_score(y_true, y_score,
+                                              sample_weight=sample_weight),
+        np.sum(sample_weight * samplewise_lraps) / np.sum(sample_weight))
+
+
 def test_coverage_error():
     # Toy case
     assert_almost_equal(coverage_error([[0, 1]], [[0.25, 0.75]]), 1)