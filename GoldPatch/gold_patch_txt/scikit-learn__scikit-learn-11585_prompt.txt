# 修复代码生成提示词（实例ID：scikit-learn__scikit-learn-11585）
## 代码仓库
scikit-learn/scikit-learn

## 原始问题描述
SparsePCA incorrectly scales results in .transform()
#### Description
When using `SparsePCA`, the `transform()` method incorrectly scales the results based on the *number of rows* in the data matrix passed.

#### Proposed Fix
I am regrettably unable to do a pull request from where I sit. The issue is with this chunk of code, as of writing this at [line number 179 in sparse_pca.py](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/decomposition/sparse_pca.py#L179):
```python
        U = ridge_regression(self.components_.T, X.T, ridge_alpha,
                             solver='cholesky')
        s = np.sqrt((U ** 2).sum(axis=0))
        s[s == 0] = 1
        U /= s
        return U
```
I honestly do not understand the details of the chosen implementation of SparsePCA. Depending on the objectives of the class, making use of the features as significant for unseen examples requires one of two modifications. Either (a) **learn** the scale factor `s` from the training data (i.e., make it an instance attribute like `.scale_factor_`), or (b) use `.mean(axis=0)` instead of `.sum(axis=0)` to remove the number-of-examples dependency.

#### Steps/Code to Reproduce
```python
from sklearn.decomposition import SparsePCA
import numpy as np


def get_data( count, seed ):
    np.random.seed(seed)
    col1 = np.random.random(count)
    col2 = np.random.random(count)

    data = np.hstack([ a[:,np.newaxis] for a in [
        col1 + .01*np.random.random(count),
        -col1 + .01*np.random.random(count),
        2*col1 + col2 + .01*np.random.random(count),
        col2 + .01*np.random.random(count),
        ]])
    return data


train = get_data(1000,1)
spca = SparsePCA(max_iter=20)
results_train = spca.fit_transform( train )

test = get_data(10,1)
results_test = spca.transform( test )

print( "Training statistics:" )
print( "  mean: %12.3f" % results_train.mean() )
print( "   max: %12.3f" % results_train.max() )
print( "   min: %12.3f" % results_train.min() )
print( "Testing statistics:" )
print( "  mean: %12.3f" % results_test.mean() )
print( "   max: %12.3f" % results_test.max() )
print( "   min: %12.3f" % results_test.min() )
```
Output:
```
Training statistics:
  mean:       -0.009
   max:        0.067
   min:       -0.080
Testing statistics:
  mean:       -0.107
   max:        0.260
   min:       -0.607
```

#### Expected Results
The test results min/max values are on the same scale as the training results.

#### Actual Results
The test results min/max values are much larger than the training results, because fewer examples were used. It is trivial to repeat this process with various sizes of training and testing data to see the relationship.



## 参考黄金补丁（正确的修复方案）
diff --git a/sklearn/decomposition/tests/test_sparse_pca.py b/sklearn/decomposition/tests/test_sparse_pca.py
--- a/sklearn/decomposition/tests/test_sparse_pca.py
+++ b/sklearn/decomposition/tests/test_sparse_pca.py
@@ -2,19 +2,20 @@
 # License: BSD 3 clause
 
 import sys
+import pytest
 
 import numpy as np
 
 from sklearn.utils.testing import assert_array_almost_equal
 from sklearn.utils.testing import assert_equal
-from sklearn.utils.testing import assert_array_equal
+from sklearn.utils.testing import assert_allclose
 from sklearn.utils.testing import SkipTest
 from sklearn.utils.testing import assert_true
 from sklearn.utils.testing import assert_false
 from sklearn.utils.testing import assert_warns_message
 from sklearn.utils.testing import if_safe_multiprocessing_with_blas
 
-from sklearn.decomposition import SparsePCA, MiniBatchSparsePCA
+from sklearn.decomposition import SparsePCA, MiniBatchSparsePCA, PCA
 from sklearn.utils import check_random_state
 
 
@@ -43,31 +44,37 @@ def generate_toy_data(n_components, n_samples, image_size, random_state=None):
 # test different aspects of the code in the same test
 
 
-def test_correct_shapes():
+@pytest.mark.filterwarnings("ignore:normalize_components")
+@pytest.mark.parametrize("norm_comp", [False, True])
+def test_correct_shapes(norm_comp):
     rng = np.random.RandomState(0)
     X = rng.randn(12, 10)
-    spca = SparsePCA(n_components=8, random_state=rng)
+    spca = SparsePCA(n_components=8, random_state=rng,
+                     normalize_components=norm_comp)
     U = spca.fit_transform(X)
     assert_equal(spca.components_.shape, (8, 10))
     assert_equal(U.shape, (12, 8))
     # test overcomplete decomposition
-    spca = SparsePCA(n_components=13, random_state=rng)
+    spca = SparsePCA(n_components=13, random_state=rng,
+                     normalize_components=norm_comp)
     U = spca.fit_transform(X)
     assert_equal(spca.components_.shape, (13, 10))
     assert_equal(U.shape, (12, 13))
 
 
-def test_fit_transform():
+@pytest.mark.filterwarnings("ignore:normalize_components")
+@pytest.mark.parametrize("norm_comp", [False, True])
+def test_fit_transform(norm_comp):
     alpha = 1
     rng = np.random.RandomState(0)
     Y, _, _ = generate_toy_data(3, 10, (8, 8), random_state=rng)  # wide array
     spca_lars = SparsePCA(n_components=3, method='lars', alpha=alpha,
-                          random_state=0)
+                          random_state=0, normalize_components=norm_comp)
     spca_lars.fit(Y)
 
     # Test that CD gives similar results
     spca_lasso = SparsePCA(n_components=3, method='cd', random_state=0,
-                           alpha=alpha)
+                           alpha=alpha, normalize_components=norm_comp)
     spca_lasso.fit(Y)
     assert_array_almost_equal(spca_lasso.components_, spca_lars.components_)
 
@@ -79,75 +86,95 @@ def test_fit_transform():
                          Y, ridge_alpha=None)
 
 
+@pytest.mark.filterwarnings("ignore:normalize_components")
+@pytest.mark.parametrize("norm_comp", [False, True])
 @if_safe_multiprocessing_with_blas
-def test_fit_transform_parallel():
+def test_fit_transform_parallel(norm_comp):
     alpha = 1
     rng = np.random.RandomState(0)
     Y, _, _ = generate_toy_data(3, 10, (8, 8), random_state=rng)  # wide array
     spca_lars = SparsePCA(n_components=3, method='lars', alpha=alpha,
-                          random_state=0)
+                          random_state=0, normalize_components=norm_comp)
     spca_lars.fit(Y)
     U1 = spca_lars.transform(Y)
     # Test multiple CPUs
     spca = SparsePCA(n_components=3, n_jobs=2, method='lars', alpha=alpha,
-                     random_state=0).fit(Y)
+                     random_state=0, normalize_components=norm_comp).fit(Y)
     U2 = spca.transform(Y)
     assert_true(not np.all(spca_lars.components_ == 0))
     assert_array_almost_equal(U1, U2)
 
 
-def test_transform_nan():
+@pytest.mark.filterwarnings("ignore:normalize_components")
+@pytest.mark.parametrize("norm_comp", [False, True])
+def test_transform_nan(norm_comp):
     # Test that SparsePCA won't return NaN when there is 0 feature in all
     # samples.
     rng = np.random.RandomState(0)
     Y, _, _ = generate_toy_data(3, 10, (8, 8), random_state=rng)  # wide array
     Y[:, 0] = 0
-    estimator = SparsePCA(n_components=8)
+    estimator = SparsePCA(n_components=8, normalize_components=norm_comp)
     assert_false(np.any(np.isnan(estimator.fit_transform(Y))))
 
 
-def test_fit_transform_tall():
+@pytest.mark.filterwarnings("ignore:normalize_components")
+@pytest.mark.parametrize("norm_comp", [False, True])
+def test_fit_transform_tall(norm_comp):
     rng = np.random.RandomState(0)
     Y, _, _ = generate_toy_data(3, 65, (8, 8), random_state=rng)  # tall array
     spca_lars = SparsePCA(n_components=3, method='lars',
-                          random_state=rng)
+                          random_state=rng, normalize_components=norm_comp)
     U1 = spca_lars.fit_transform(Y)
-    spca_lasso = SparsePCA(n_components=3, method='cd', random_state=rng)
+    spca_lasso = SparsePCA(n_components=3, method='cd',
+                           random_state=rng, normalize_components=norm_comp)
     U2 = spca_lasso.fit(Y).transform(Y)
     assert_array_almost_equal(U1, U2)
 
 
-def test_initialization():
+@pytest.mark.filterwarnings("ignore:normalize_components")
+@pytest.mark.parametrize("norm_comp", [False, True])
+def test_initialization(norm_comp):
     rng = np.random.RandomState(0)
     U_init = rng.randn(5, 3)
     V_init = rng.randn(3, 4)
     model = SparsePCA(n_components=3, U_init=U_init, V_init=V_init, max_iter=0,
-                      random_state=rng)
+                      random_state=rng, normalize_components=norm_comp)
     model.fit(rng.randn(5, 4))
-    assert_array_equal(model.components_, V_init)
+    if norm_comp:
+        assert_allclose(model.components_,
+                        V_init / np.linalg.norm(V_init, axis=1)[:, None])
+    else:
+        assert_allclose(model.components_, V_init)
 
 
-def test_mini_batch_correct_shapes():
+@pytest.mark.filterwarnings("ignore:normalize_components")
+@pytest.mark.parametrize("norm_comp", [False, True])
+def test_mini_batch_correct_shapes(norm_comp):
     rng = np.random.RandomState(0)
     X = rng.randn(12, 10)
-    pca = MiniBatchSparsePCA(n_components=8, random_state=rng)
+    pca = MiniBatchSparsePCA(n_components=8, random_state=rng,
+                             normalize_components=norm_comp)
     U = pca.fit_transform(X)
     assert_equal(pca.components_.shape, (8, 10))
     assert_equal(U.shape, (12, 8))
     # test overcomplete decomposition
-    pca = MiniBatchSparsePCA(n_components=13, random_state=rng)
+    pca = MiniBatchSparsePCA(n_components=13, random_state=rng,
+                             normalize_components=norm_comp)
     U = pca.fit_transform(X)
     assert_equal(pca.components_.shape, (13, 10))
     assert_equal(U.shape, (12, 13))
 
 
-def test_mini_batch_fit_transform():
+@pytest.mark.filterwarnings("ignore:normalize_components")
+@pytest.mark.parametrize("norm_comp", [False, True])
+def test_mini_batch_fit_transform(norm_comp):
     raise SkipTest("skipping mini_batch_fit_transform.")
     alpha = 1
     rng = np.random.RandomState(0)
     Y, _, _ = generate_toy_data(3, 10, (8, 8), random_state=rng)  # wide array
     spca_lars = MiniBatchSparsePCA(n_components=3, random_state=0,
-                                   alpha=alpha).fit(Y)
+                                   alpha=alpha,
+                                   normalize_components=norm_comp).fit(Y)
     U1 = spca_lars.transform(Y)
     # Test multiple CPUs
     if sys.platform == 'win32':  # fake parallelism for win32
@@ -155,16 +182,59 @@ def test_mini_batch_fit_transform():
         _mp = joblib_par.multiprocessing
         joblib_par.multiprocessing = None
         try:
-            U2 = MiniBatchSparsePCA(n_components=3, n_jobs=2, alpha=alpha,
-                                    random_state=0).fit(Y).transform(Y)
+            spca = MiniBatchSparsePCA(n_components=3, n_jobs=2, alpha=alpha,
+                                      random_state=0,
+                                      normalize_components=norm_comp)
+            U2 = spca.fit(Y).transform(Y)
         finally:
             joblib_par.multiprocessing = _mp
     else:  # we can efficiently use parallelism
-        U2 = MiniBatchSparsePCA(n_components=3, n_jobs=2, alpha=alpha,
-                                random_state=0).fit(Y).transform(Y)
+        spca = MiniBatchSparsePCA(n_components=3, n_jobs=2, alpha=alpha,
+                                  random_state=0,
+                                  normalize_components=norm_comp)
+        U2 = spca.fit(Y).transform(Y)
     assert_true(not np.all(spca_lars.components_ == 0))
     assert_array_almost_equal(U1, U2)
     # Test that CD gives similar results
     spca_lasso = MiniBatchSparsePCA(n_components=3, method='cd', alpha=alpha,
-                                    random_state=0).fit(Y)
+                                    random_state=0,
+                                    normalize_components=norm_comp).fit(Y)
     assert_array_almost_equal(spca_lasso.components_, spca_lars.components_)
+
+
+def test_scaling_fit_transform():
+    alpha = 1
+    rng = np.random.RandomState(0)
+    Y, _, _ = generate_toy_data(3, 1000, (8, 8), random_state=rng)
+    spca_lars = SparsePCA(n_components=3, method='lars', alpha=alpha,
+                          random_state=rng, normalize_components=True)
+    results_train = spca_lars.fit_transform(Y)
+    results_test = spca_lars.transform(Y[:10])
+    assert_allclose(results_train[0], results_test[0])
+
+
+def test_pca_vs_spca():
+    rng = np.random.RandomState(0)
+    Y, _, _ = generate_toy_data(3, 1000, (8, 8), random_state=rng)
+    Z, _, _ = generate_toy_data(3, 10, (8, 8), random_state=rng)
+    spca = SparsePCA(alpha=0, ridge_alpha=0, n_components=2,
+                     normalize_components=True)
+    pca = PCA(n_components=2)
+    pca.fit(Y)
+    spca.fit(Y)
+    results_test_pca = pca.transform(Z)
+    results_test_spca = spca.transform(Z)
+    assert_allclose(np.abs(spca.components_.dot(pca.components_.T)),
+                    np.eye(2), atol=1e-5)
+    results_test_pca *= np.sign(results_test_pca[0, :])
+    results_test_spca *= np.sign(results_test_spca[0, :])
+    assert_allclose(results_test_pca, results_test_spca)
+
+
+@pytest.mark.parametrize("spca", [SparsePCA, MiniBatchSparsePCA])
+def test_spca_deprecation_warning(spca):
+    rng = np.random.RandomState(0)
+    Y, _, _ = generate_toy_data(3, 10, (8, 8), random_state=rng)
+    warn_message = "normalize_components"
+    assert_warns_message(DeprecationWarning, warn_message,
+                         spca(normalize_components=False).fit, Y)