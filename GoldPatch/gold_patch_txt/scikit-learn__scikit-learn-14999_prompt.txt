# 修复代码生成提示词（实例ID：scikit-learn__scikit-learn-14999）
## 代码仓库
scikit-learn/scikit-learn

## 原始问题描述
data leak in GBDT due to warm start
(This is about the non-histogram-based version of GBDTs)

X is split into train and validation data with `train_test_split(random_state=self.random_state)`.

As @johannfaouzi noted, in a warm starting context, this will produce a leak if If `self.random_state` is a `RandomState` instance: some samples that were used for training in a previous `fit` might be used for validation now.

~~I think the right fix would be to raise a `ValueError` if the provided random state isn't a number and early-stopping is activated~~


## 参考黄金补丁（正确的修复方案）
diff --git a/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py b/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py
--- a/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py
+++ b/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py
@@ -154,13 +154,15 @@ def test_warm_start_clear(GradientBoosting, X, y):
     (HistGradientBoostingClassifier, X_classification, y_classification),
     (HistGradientBoostingRegressor, X_regression, y_regression)
 ])
-@pytest.mark.parametrize('rng_type', ('int', 'instance'))
+@pytest.mark.parametrize('rng_type', ('none', 'int', 'instance'))
 def test_random_seeds_warm_start(GradientBoosting, X, y, rng_type):
     # Make sure the seeds for train/val split and small trainset subsampling
     # are correctly set in a warm start context.
     def _get_rng(rng_type):
         # Helper to avoid consuming rngs
-        if rng_type == 'int':
+        if rng_type == 'none':
+            return None
+        elif rng_type == 'int':
             return 42
         else:
             return np.random.RandomState(0)
@@ -169,22 +171,30 @@ def _get_rng(rng_type):
     gb_1 = GradientBoosting(n_iter_no_change=5, max_iter=2,
                             random_state=random_state)
     gb_1.fit(X, y)
-    train_val_seed_1 = gb_1._train_val_split_seed
-    small_trainset_seed_1 = gb_1._small_trainset_seed
+    random_seed_1_1 = gb_1._random_seed
+
+    gb_1.fit(X, y)
+    random_seed_1_2 = gb_1._random_seed  # clear the old state, different seed
 
     random_state = _get_rng(rng_type)
     gb_2 = GradientBoosting(n_iter_no_change=5, max_iter=2,
                             random_state=random_state, warm_start=True)
     gb_2.fit(X, y)  # inits state
-    train_val_seed_2 = gb_2._train_val_split_seed
-    small_trainset_seed_2 = gb_2._small_trainset_seed
+    random_seed_2_1 = gb_2._random_seed
     gb_2.fit(X, y)  # clears old state and equals est
-    train_val_seed_3 = gb_2._train_val_split_seed
-    small_trainset_seed_3 = gb_2._small_trainset_seed
-
-    # Check that all seeds are equal
-    assert train_val_seed_1 == train_val_seed_2
-    assert small_trainset_seed_1 == small_trainset_seed_2
-
-    assert train_val_seed_2 == train_val_seed_3
-    assert small_trainset_seed_2 == small_trainset_seed_3
+    random_seed_2_2 = gb_2._random_seed
+
+    # Without warm starting, the seeds should be
+    # * all different if random state is None
+    # * all equal if random state is an integer
+    # * different when refitting and equal with a new estimator (because
+    #   the random state is mutated)
+    if rng_type == 'none':
+        assert random_seed_1_1 != random_seed_1_2 != random_seed_2_1
+    elif rng_type == 'int':
+        assert random_seed_1_1 == random_seed_1_2 == random_seed_2_1
+    else:
+        assert random_seed_1_1 == random_seed_2_1 != random_seed_1_2
+
+    # With warm starting, the seeds must be equal
+    assert random_seed_2_1 == random_seed_2_2