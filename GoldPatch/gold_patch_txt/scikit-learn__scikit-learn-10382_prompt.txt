# 修复代码生成提示词（实例ID：scikit-learn__scikit-learn-10382）
## 代码仓库
scikit-learn/scikit-learn

## 原始问题描述
TfidfVectorizer dtype argument ignored
#### Description
TfidfVectorizer's fit/fit_transform output is always np.float64 instead of the specified dtype

#### Steps/Code to Reproduce
```py
from sklearn.feature_extraction.text import TfidfVectorizer
test = TfidfVectorizer(dtype=np.float32)
print(test.fit_transform(["Help I have a bug"]).dtype)
```

#### Expected Results
```py
dtype('float32')
```

#### Actual Results
```py
dtype('float64')
```

#### Versions
```
Darwin-17.2.0-x86_64-i386-64bit
Python 3.6.1 |Anaconda 4.4.0 (x86_64)| (default, May 11 2017, 13:04:09) 
[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)]
NumPy 1.13.3
SciPy 1.0.0
Scikit-Learn 0.19.0
```
  
FitFailedWarning raised by cross validation could do with better message
Current message says "Classifier fit failed" but the estimator is not necessarily a classifier. (Perhaps that's too pedantic of me...)

`%r` is not technically the best way to display an error message. We could either use `traceback.format_exc` and include the whole traceback; or we can use `traceback.format_exception_only` to print it properly (though I think this is the same as `"%s: %s" % (type(exc), exc)`).

Another thing we can consider, now that `_fit_and_score` provides structured results to `*SearchCV` and `cross_validate`, is to store the full traceback in some array of `*SearchCV.cv_results_`.


## 参考黄金补丁（正确的修复方案）
diff --git a/sklearn/model_selection/tests/test_validation.py b/sklearn/model_selection/tests/test_validation.py
--- a/sklearn/model_selection/tests/test_validation.py
+++ b/sklearn/model_selection/tests/test_validation.py
@@ -9,6 +9,9 @@
 
 import numpy as np
 from scipy.sparse import coo_matrix, csr_matrix
+from sklearn.exceptions import FitFailedWarning
+
+from sklearn.tests.test_grid_search import FailingClassifier
 
 from sklearn.utils.testing import assert_true
 from sklearn.utils.testing import assert_false
@@ -40,6 +43,7 @@
 from sklearn.model_selection import learning_curve
 from sklearn.model_selection import validation_curve
 from sklearn.model_selection._validation import _check_is_permutation
+from sklearn.model_selection._validation import _fit_and_score
 
 from sklearn.datasets import make_regression
 from sklearn.datasets import load_boston
@@ -1421,3 +1425,27 @@ def test_permutation_test_score_pandas():
         check_series = lambda x: isinstance(x, TargetType)
         clf = CheckingClassifier(check_X=check_df, check_y=check_series)
         permutation_test_score(clf, X_df, y_ser)
+
+
+def test_fit_and_score():
+    # Create a failing classifier to deliberately fail
+    failing_clf = FailingClassifier(FailingClassifier.FAILING_PARAMETER)
+    # dummy X data
+    X = np.arange(1, 10)
+    fit_and_score_args = [failing_clf, X, None, dict(), None, None, 0,
+                          None, None]
+    # passing error score to trigger the warning message
+    fit_and_score_kwargs = {'error_score': 0}
+    # check if the warning message type is as expected
+    assert_warns(FitFailedWarning, _fit_and_score, *fit_and_score_args,
+                 **fit_and_score_kwargs)
+    # since we're using FailingClassfier, our error will be the following
+    error_message = "ValueError: Failing classifier failed as required"
+    # the warning message we're expecting to see
+    warning_message = ("Estimator fit failed. The score on this train-test "
+                       "partition for these parameters will be set to %f. "
+                       "Details: \n%s" % (fit_and_score_kwargs['error_score'],
+                                          error_message))
+    # check if the same warning is triggered
+    assert_warns_message(FitFailedWarning, warning_message, _fit_and_score,
+                         *fit_and_score_args, **fit_and_score_kwargs)