# 修复代码生成提示词（实例ID：scikit-learn__scikit-learn-10377）
## 代码仓库
scikit-learn/scikit-learn

## 原始问题描述
BUG Inconsistent f1_score behavior when combining label indicator input with labels attribute
#### Description
When using label indicator inputs for y_pred and y_true, metrics.f1_score calculates the macro average over all label-specific f-scores whenever the labels parameter includes column index 0. It should only average over the label-specific scores indicated by the labels parameter, as it does when 0 is not present in the labels parameter.

<!-- Instructions For Filing a Bug: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#filing-bugs -->

#### Steps/Code to Reproduce
```python
import numpy as np
from sklearn.metrics import f1_score, precision_recall_fscore_support

y_true = np.array([[0, 1, 0, 0],
                   [1, 0, 0, 0],
                   [1, 0, 0, 0]])
y_pred = np.array([[0, 1, 0, 0],
                   [0, 0, 1, 0],
                   [0, 1, 0, 0]])

p, r, f, s = precision_recall_fscore_support(y_true, y_pred)
print(f)
print(f1_score(y_true, y_pred, labels=[0,1], average='macro'))
print(f1_score(y_true, y_pred, labels=[0,1,2], average='macro'))
print(f1_score(y_true, y_pred, labels=[1,3], average='macro'))
print(f1_score(y_true, y_pred, labels=[1,2,3], average='macro'))
```
#### Expected Results
```
[ 0.          0.66666667  0.          0.        ]
0.333333333333
0.222222222222
0.333333333333
0.222222222222
```
#### Actual Results
```
[ 0.          0.66666667  0.          0.        ]
0.166666666667
0.166666666667
0.333333333333
0.222222222222
```

<!-- Please paste or specifically describe the actual output or traceback. -->

#### Versions
Windows-7-6.1.7601-SP1
Python 3.5.3 |Anaconda custom (64-bit)| (default, May 15 2017, 10:43:23) [MSC v.1900 64 bit (AMD64)]
NumPy 1.13.1
SciPy 0.19.0
Scikit-Learn 0.19.0

<!-- Thanks for contributing! -->



## 参考黄金补丁（正确的修复方案）
diff --git a/sklearn/metrics/tests/test_classification.py b/sklearn/metrics/tests/test_classification.py
--- a/sklearn/metrics/tests/test_classification.py
+++ b/sklearn/metrics/tests/test_classification.py
@@ -197,6 +197,14 @@ def test_precision_recall_f_extra_labels():
         assert_raises(ValueError, recall_score, y_true_bin, y_pred_bin,
                       labels=np.arange(-1, 4), average=average)
 
+    # tests non-regression on issue #10307
+    y_true = np.array([[0, 1, 1], [1, 0, 0]])
+    y_pred = np.array([[1, 1, 1], [1, 0, 1]])
+    p, r, f, _ = precision_recall_fscore_support(y_true, y_pred,
+                                                 average='samples',
+                                                 labels=[0, 1])
+    assert_almost_equal(np.array([p, r, f]), np.array([3 / 4, 1, 5 / 6]))
+
 
 @ignore_warnings
 def test_precision_recall_f_ignored_labels():