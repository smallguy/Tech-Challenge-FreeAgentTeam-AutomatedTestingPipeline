# 修复代码生成提示词（实例ID：scikit-learn__scikit-learn-13974）
## 代码仓库
scikit-learn/scikit-learn

## 原始问题描述
Design of add_indicator in SimpleImputer may fail when running cross validation
<!--
If your issue is a usage question, submit it here instead:
- StackOverflow with the scikit-learn tag: https://stackoverflow.com/questions/tagged/scikit-learn
- Mailing List: https://mail.python.org/mailman/listinfo/scikit-learn
For more information, see User Questions: http://scikit-learn.org/stable/support.html#user-questions
-->

<!-- Instructions For Filing a Bug: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#filing-bugs -->

#### Description
<!-- Example: Joblib Error thrown when calling fit on LatentDirichletAllocation with evaluate_every > 0-->
The design of `add_indicator` depends on missing values exist in the training data. This will break cross validation.

#### Steps/Code to Reproduce
<!--
Example:
```python
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

docs = ["Help I have a bug" for i in range(1000)]

vectorizer = CountVectorizer(input=docs, analyzer='word')
lda_features = vectorizer.fit_transform(docs)

lda_model = LatentDirichletAllocation(
    n_topics=10,
    learning_method='online',
    evaluate_every=10,
    n_jobs=4,
)
model = lda_model.fit(lda_features)
```
If the code is too long, feel free to put it in a public gist and link
it in the issue: https://gist.github.com
-->
```py
import numpy as np
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import PredefinedSplit
from sklearn.model_selection import cross_val_score
from sklearn.pipeline import make_pipeline

X = np.array([[1, 2, 3, np.nan]]).T
y = np.array([0, 0, 1, 1])
test_fold = np.array([0, 1, 0, 1])

ps = PredefinedSplit(test_fold)
pipe1 = make_pipeline(SimpleImputer(add_indicator=True), 
                      LogisticRegression(solver='lbfgs'))

cross_val_score(pipe1, X, y, cv=ps)
```

#### Expected Results
<!-- Example: No error is thrown. Please paste or describe the expected results.-->
No error is thrown.

#### Actual Results
<!-- Please paste or specifically describe the actual output or traceback. -->
```
ValueError: The features [0] have missing values in transform 
but have no missing values in fit.
```

#### Thoughts

The original design was adopted because, if the training data has no missing value, there will be a column with all zeros. This type of error will appear when we try to do grid search over the `add_indicator` parameter. One way to work around this is to split the data in such a way that missing values are available (for each column that has a missing value) in both the training set and test set.

<!-- Thanks for contributing! -->


## 参考黄金补丁（正确的修复方案）
diff --git a/sklearn/impute/tests/test_impute.py b/sklearn/impute/tests/test_impute.py
--- a/sklearn/impute/tests/test_impute.py
+++ b/sklearn/impute/tests/test_impute.py
@@ -445,6 +445,16 @@ def test_imputation_constant_pandas(dtype):
     assert_array_equal(X_trans, X_true)
 
 
+@pytest.mark.parametrize('Imputer', (SimpleImputer, IterativeImputer))
+def test_imputation_missing_value_in_test_array(Imputer):
+    # [Non Regression Test for issue #13968] Missing value in test set should
+    # not throw an error and return a finite dataset
+    train = [[1], [2]]
+    test = [[3], [np.nan]]
+    imputer = Imputer(add_indicator=True)
+    imputer.fit(train).transform(test)
+
+
 def test_imputation_pipeline_grid_search():
     # Test imputation within a pipeline + gridsearch.
     X = sparse_random_matrix(100, 100, density=0.10)