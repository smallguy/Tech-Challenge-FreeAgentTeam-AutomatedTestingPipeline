# 修复代码生成提示词（实例ID：scikit-learn__scikit-learn-11160）
## 代码仓库
scikit-learn/scikit-learn

## 原始问题描述
`classification_report` output options? 
Is it possible to add output options to http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html.  It would be really useful to have a `pd.DataFrame` output or `xr.DataArray` output.  Right now it outputs as a string that must be printed but it's difficult to use the results.  I can make a quick helper script if that could be useful? 
[MRG] Classification report Dict-of-Dicts output
<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#Contributing-Pull-Requests
-->
#### Reference Issue
<!-- Example: Fixes #1234 -->
Fixes #7845 

#### What does this implement/fix? Explain your changes.
This PR adds an option of returning the classification report in the form of a Dictionary of Dictionaries.

#### Any other comments?
Will add tests for the code, if the code is approved.


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->



## 参考黄金补丁（正确的修复方案）
diff --git a/sklearn/metrics/tests/test_classification.py b/sklearn/metrics/tests/test_classification.py
--- a/sklearn/metrics/tests/test_classification.py
+++ b/sklearn/metrics/tests/test_classification.py
@@ -12,7 +12,7 @@
 from sklearn.datasets import make_multilabel_classification
 from sklearn.preprocessing import label_binarize
 from sklearn.utils.validation import check_random_state
-
+from sklearn.utils.testing import assert_dict_equal
 from sklearn.utils.testing import assert_raises, clean_warning_registry
 from sklearn.utils.testing import assert_raise_message
 from sklearn.utils.testing import assert_equal
@@ -101,6 +101,36 @@ def make_prediction(dataset=None, binary=False):
 ###############################################################################
 # Tests
 
+def test_classification_report_dictionary_output():
+
+    # Test performance report with dictionary output
+    iris = datasets.load_iris()
+    y_true, y_pred, _ = make_prediction(dataset=iris, binary=False)
+
+    # print classification report with class names
+    expected_report = {'setosa': {'precision': 0.82608695652173914,
+                                  'recall': 0.79166666666666663,
+                                  'f1-score': 0.8085106382978724,
+                                  'support': 24},
+                       'versicolor': {'precision': 0.33333333333333331,
+                                      'recall': 0.096774193548387094,
+                                      'f1-score': 0.15000000000000002,
+                                      'support': 31},
+                       'virginica': {'precision': 0.41860465116279072,
+                                     'recall': 0.90000000000000002,
+                                     'f1-score': 0.57142857142857151,
+                                     'support': 20},
+                       'avg / total': {'precision': 0.51375351084147847,
+                                       'recall': 0.53333333333333333,
+                                       'f1-score': 0.47310435663627154,
+                                       'support': 75}}
+
+    report = classification_report(
+        y_true, y_pred, labels=np.arange(len(iris.target_names)),
+        target_names=iris.target_names, output_dict=True)
+
+    assert_dict_equal(report, expected_report)
+
 
 def test_multilabel_accuracy_score_subset_accuracy():
     # Dense label indicator matrix format