# 修复代码生成提示词（实例ID：scikit-learn__scikit-learn-25443）
## 代码仓库
scikit-learn/scikit-learn

## 原始问题描述
With MLPClassifer, when warm_start is True or coeffs_ are provided, fit doesn’t respect max_iters
#### Description
With MLPClassifer, when warm_start is True or coeffs_ are provided, fit doesn’t respect max_iters. The reason for this is, when fitting, max iteration check is equality (==) against self.n_iter_. When warm_start is true or coeffs_ are provided, initialize is not called; this method resets n_iter_ to 0. Based on this implementation, there is doubt as to the meaning of max_iter. Consider, if max_iter is 1 and fit terminates due to reaching maximum iterations, subsequent fittings with warm_start true will never terminate due to reaching maximum iterations. This is bug. An alternate interpretation is max_iter represents the maximum iterations per fit call. In this case, the implementation is also wrong. The later interpretation seems more reasonable.

#### Steps/Code to Reproduce
```
import numpy as np
from sklearn.neural_network import MLPClassifier

X = np.random.rand(100,10)
y = np.random.random_integers(0, 1, (100,))

clf = MLPClassifier(max_iter=1, warm_start=True, verbose=True)
for k in range(3):
    clf.fit(X, y)
```
#### Expected Results
Iteration 1, loss = 0.72311215
ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.
Iteration 2, loss = 0.71843526
ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.
Iteration 3, loss = 0.71418678
ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.

#### Actual Results
Iteration 1, loss = 0.72311215
ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.
Iteration 2, loss = 0.71843526
Iteration 3, loss = 0.71418678

#### Versions
Windows-7-6.1.7601-SP1
Python 3.6.0 (v3.6.0:41df79263a11, Dec 23 2016, 08:06:12) [MSC v.1900 64 bit (AMD64)]
NumPy 1.12.0
SciPy 0.18.1
Scikit-Learn 0.18.1




## 参考黄金补丁（正确的修复方案）
diff --git a/sklearn/neural_network/tests/test_mlp.py b/sklearn/neural_network/tests/test_mlp.py
--- a/sklearn/neural_network/tests/test_mlp.py
+++ b/sklearn/neural_network/tests/test_mlp.py
@@ -752,7 +752,7 @@ def test_warm_start_full_iteration(MLPEstimator):
     clf.fit(X, y)
     assert max_iter == clf.n_iter_
     clf.fit(X, y)
-    assert 2 * max_iter == clf.n_iter_
+    assert max_iter == clf.n_iter_
 
 
 def test_n_iter_no_change():
@@ -926,3 +926,25 @@ def test_mlp_warm_start_with_early_stopping(MLPEstimator):
     mlp.set_params(max_iter=20)
     mlp.fit(X_iris, y_iris)
     assert len(mlp.validation_scores_) > n_validation_scores
+
+
+@pytest.mark.parametrize("MLPEstimator", [MLPClassifier, MLPRegressor])
+@pytest.mark.parametrize("solver", ["sgd", "adam", "lbfgs"])
+def test_mlp_warm_start_no_convergence(MLPEstimator, solver):
+    """Check that we stop the number of iteration at `max_iter` when warm starting.
+
+    Non-regression test for:
+    https://github.com/scikit-learn/scikit-learn/issues/24764
+    """
+    model = MLPEstimator(
+        solver=solver, warm_start=True, early_stopping=False, max_iter=10
+    )
+
+    with pytest.warns(ConvergenceWarning):
+        model.fit(X_iris, y_iris)
+    assert model.n_iter_ == 10
+
+    model.set_params(max_iter=20)
+    with pytest.warns(ConvergenceWarning):
+        model.fit(X_iris, y_iris)
+    assert model.n_iter_ == 20