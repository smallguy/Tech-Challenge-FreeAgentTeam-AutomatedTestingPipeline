# 修复代码生成提示词（实例ID：scikit-learn__scikit-learn-13933）
## 代码仓库
scikit-learn/scikit-learn

## 原始问题描述
GBDTs should bin train and validation data separately? 
In the new GBDTs we bin the data before calling `train_test_split()` (for early-stopping).

That means that the validation set is also used to find the bin thresholds (it is of course not used to find the split points!).

I feel like the "data leak" is very minimal, but it seems more correct to bin X_train and X_val separately.

@ogrisel WDYT?


## 参考黄金补丁（正确的修复方案）
diff --git a/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py b/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py
--- a/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py
+++ b/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py
@@ -6,6 +6,7 @@
 from sklearn.experimental import enable_hist_gradient_boosting  # noqa
 from sklearn.ensemble import HistGradientBoostingRegressor
 from sklearn.ensemble import HistGradientBoostingClassifier
+from sklearn.ensemble._hist_gradient_boosting.binning import _BinMapper
 
 
 X_classification, y_classification = make_classification(random_state=0)
@@ -145,3 +146,29 @@ def test_should_stop(scores, n_iter_no_change, tol, stopping):
         n_iter_no_change=n_iter_no_change, tol=tol
     )
     assert gbdt._should_stop(scores) == stopping
+
+
+def test_binning_train_validation_are_separated():
+    # Make sure training and validation data are binned separately.
+    # See issue 13926
+
+    rng = np.random.RandomState(0)
+    validation_fraction = .2
+    gb = HistGradientBoostingClassifier(
+        n_iter_no_change=5,
+        validation_fraction=validation_fraction,
+        random_state=rng
+    )
+    gb.fit(X_classification, y_classification)
+    mapper_training_data = gb.bin_mapper_
+
+    # Note that since the data is small there is no subsampling and the
+    # random_state doesn't matter
+    mapper_whole_data = _BinMapper(random_state=0)
+    mapper_whole_data.fit(X_classification)
+
+    n_samples = X_classification.shape[0]
+    assert np.all(mapper_training_data.actual_n_bins_ ==
+                  int((1 - validation_fraction) * n_samples))
+    assert np.all(mapper_training_data.actual_n_bins_ !=
+                  mapper_whole_data.actual_n_bins_)