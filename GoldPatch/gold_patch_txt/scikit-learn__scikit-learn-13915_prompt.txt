# ‰øÆÂ§ç‰ª£Á†ÅÁîüÊàêÊèêÁ§∫ËØçÔºàÂÆû‰æãIDÔºöscikit-learn__scikit-learn-13915Ôºâ
## ‰ª£Á†Å‰ªìÂ∫ì
scikit-learn/scikit-learn

## ÂéüÂßãÈóÆÈ¢òÊèèËø∞
utils.sparsefuncs.min_max_axis gives TypeError when input is large csc matrix when OS is 32 bit Windows
#### Description
On 32 bit versions of Windows, when `min_max_axis` is called on a csc matrix where `indptr.dtype` is int64, an error is produced. This prevents [this](https://github.com/scikit-learn/scikit-learn/pull/13704/) pull request passing tests (see [here](https://github.com/scikit-learn/scikit-learn/pull/13704/checks?check_run_id=109958355)).

#### Steps/Code to Reproduce
```python
import scipy.sparse as sp
from sklearn.utils.sparsefuncs import min_max_axis

X = sp.csc_matrix([[1,2],[3,4]])
X.indptr = X.indptr.astype('int64')

Y = sp.csr_matrix([[1,2],[3,4]])
Y.indptr = Y.indptr.astype('int64')

print(min_max_axis(Y, 0))
print(min_max_axis(X, 0))
```

#### Expected Results
```
(array([1, 2], dtype=int32), array([3, 4], dtype=int32))
(array([1, 2], dtype=int32), array([3, 4], dtype=int32))
```

#### Actual Results
```
(array([1, 2], dtype=int32), array([3, 4], dtype=int32))
Traceback (most recent call last):
  File "C:\Users\rod\bug.py", line 12, in <module>
    print(min_max_axis(X, 0))
  File "C:\Users\rod\AppData\Local\Programs\Python\Python35-32\lib\site-packages\sklearn\utils\sparsefuncs.py", line 434, in min_max_axis
    return _sparse_min_max(X, axis=axis)
  File "C:\Users\rod\AppData\Local\Programs\Python\Python35-32\lib\site-packages\sklearn\utils\sparsefuncs.py", line 395, in _sparse_min_max
    return (_sparse_min_or_max(X, axis, np.minimum),
  File "C:\Users\rod\AppData\Local\Programs\Python\Python35-32\lib\site-packages\sklearn\utils\sparsefuncs.py", line 389, in _sparse_min_or_max
    return _min_or_max_axis(X, axis, min_or_max)
  File "C:\Users\rod\AppData\Local\Programs\Python\Python35-32\lib\site-packages\sklearn\utils\sparsefuncs.py", line 359, in _min_or_max_axis
    major_index, value = _minor_reduce(mat, min_or_max)
  File "C:\Users\rod\AppData\Local\Programs\Python\Python35-32\lib\site-packages\sklearn\utils\sparsefuncs.py", line 344, in _minor_reduce
    value = ufunc.reduceat(X.data, X.indptr[major_index])
TypeError: Cannot cast array data from dtype('int64') to dtype('int32') according to the rule 'safe'
```

#### Versions
System:
    python: 3.5.4 (v3.5.4:3f56838, Aug  8 2017, 02:07:06) [MSC v.1900 32 bit (Intel)]
   machine: Windows-10-10.0.17763-SP0
executable: C:\Users\rod\AppData\Local\Programs\Python\Python35-32\pythonw.exe

BLAS:
    macros: 
cblas_libs: cblas
  lib_dirs: 

Python deps:
    Cython: 0.29.7
     scipy: 1.2.1
setuptools: 28.8.0
     numpy: 1.16.3
       pip: 19.1
    pandas: None
   sklearn: 0.20.3



## ÂèÇËÄÉÈªÑÈáëË°•‰∏ÅÔºàÊ≠£Á°ÆÁöÑ‰øÆÂ§çÊñπÊ°àÔºâ
diff --git a/sklearn/cross_decomposition/tests/test_pls.py b/sklearn/cross_decomposition/tests/test_pls.py
--- a/sklearn/cross_decomposition/tests/test_pls.py
+++ b/sklearn/cross_decomposition/tests/test_pls.py
@@ -358,13 +358,13 @@ def test_scale_and_stability():
             X_score, Y_score = clf.fit_transform(X, Y)
             clf.set_params(scale=False)
             X_s_score, Y_s_score = clf.fit_transform(X_s, Y_s)
-            assert_array_almost_equal(X_s_score, X_score)
-            assert_array_almost_equal(Y_s_score, Y_score)
+            assert_array_almost_equal(X_s_score, X_score, decimal=4)
+            assert_array_almost_equal(Y_s_score, Y_score, decimal=4)
             # Scaling should be idempotent
             clf.set_params(scale=True)
             X_score, Y_score = clf.fit_transform(X_s, Y_s)
-            assert_array_almost_equal(X_s_score, X_score)
-            assert_array_almost_equal(Y_s_score, Y_score)
+            assert_array_almost_equal(X_s_score, X_score, decimal=4)
+            assert_array_almost_equal(Y_s_score, Y_score, decimal=4)
 
 
 def test_pls_errors():
diff --git a/sklearn/decomposition/tests/test_fastica.py b/sklearn/decomposition/tests/test_fastica.py
--- a/sklearn/decomposition/tests/test_fastica.py
+++ b/sklearn/decomposition/tests/test_fastica.py
@@ -3,6 +3,7 @@
 """
 import itertools
 import warnings
+import pytest
 
 import numpy as np
 from scipy import stats
@@ -50,9 +51,11 @@ def test_gs():
     assert_less((tmp[:5] ** 2).sum(), 1.e-10)
 
 
-def test_fastica_simple(add_noise=False):
+@pytest.mark.parametrize("add_noise", [True, False])
+@pytest.mark.parametrize("seed", range(1))
+def test_fastica_simple(add_noise, seed):
     # Test the FastICA algorithm on very simple data.
-    rng = np.random.RandomState(0)
+    rng = np.random.RandomState(seed)
     # scipy.stats uses the global RNG:
     n_samples = 1000
     # Generate two sources:
@@ -82,12 +85,15 @@ def g_test(x):
     whitening = [True, False]
     for algo, nl, whiten in itertools.product(algos, nls, whitening):
         if whiten:
-            k_, mixing_, s_ = fastica(m.T, fun=nl, algorithm=algo)
+            k_, mixing_, s_ = fastica(m.T, fun=nl, algorithm=algo,
+                                      random_state=rng)
             assert_raises(ValueError, fastica, m.T, fun=np.tanh,
                           algorithm=algo)
         else:
-            X = PCA(n_components=2, whiten=True).fit_transform(m.T)
-            k_, mixing_, s_ = fastica(X, fun=nl, algorithm=algo, whiten=False)
+            pca = PCA(n_components=2, whiten=True, random_state=rng)
+            X = pca.fit_transform(m.T)
+            k_, mixing_, s_ = fastica(X, fun=nl, algorithm=algo, whiten=False,
+                                      random_state=rng)
             assert_raises(ValueError, fastica, X, fun=np.tanh,
                           algorithm=algo)
         s_ = s_.T
@@ -113,8 +119,9 @@ def g_test(x):
             assert_almost_equal(np.dot(s2_, s2) / n_samples, 1, decimal=1)
 
     # Test FastICA class
-    _, _, sources_fun = fastica(m.T, fun=nl, algorithm=algo, random_state=0)
-    ica = FastICA(fun=nl, algorithm=algo, random_state=0)
+    _, _, sources_fun = fastica(m.T, fun=nl, algorithm=algo,
+                                random_state=seed)
+    ica = FastICA(fun=nl, algorithm=algo, random_state=seed)
     sources = ica.fit_transform(m.T)
     assert_equal(ica.components_.shape, (2, 2))
     assert_equal(sources.shape, (1000, 2))
@@ -125,7 +132,7 @@ def g_test(x):
     assert_equal(ica.mixing_.shape, (2, 2))
 
     for fn in [np.tanh, "exp(-.5(x^2))"]:
-        ica = FastICA(fun=fn, algorithm=algo, random_state=0)
+        ica = FastICA(fun=fn, algorithm=algo)
         assert_raises(ValueError, ica.fit, m.T)
 
     assert_raises(TypeError, FastICA(fun=range(10)).fit, m.T)
diff --git a/sklearn/linear_model/tests/test_least_angle.py b/sklearn/linear_model/tests/test_least_angle.py
--- a/sklearn/linear_model/tests/test_least_angle.py
+++ b/sklearn/linear_model/tests/test_least_angle.py
@@ -451,16 +451,23 @@ def test_lars_cv():
     assert not hasattr(lars_cv, 'n_nonzero_coefs')
 
 
-@pytest.mark.filterwarnings('ignore::FutureWarning')
-def test_lars_cv_max_iter():
-    with warnings.catch_warnings(record=True) as w:
+def test_lars_cv_max_iter(recwarn):
+    warnings.simplefilter('always')
+    with np.errstate(divide='raise', invalid='raise'):
+        X = diabetes.data
+        y = diabetes.target
         rng = np.random.RandomState(42)
         x = rng.randn(len(y))
         X = diabetes.data
         X = np.c_[X, x, x]  # add correlated features
-        lars_cv = linear_model.LassoLarsCV(max_iter=5)
+        lars_cv = linear_model.LassoLarsCV(max_iter=5, cv=5)
         lars_cv.fit(X, y)
-    assert len(w) == 0
+    # Check that there is no warning in general and no ConvergenceWarning
+    # in particular.
+    # Materialize the string representation of the warning to get a more
+    # informative error message in case of AssertionError.
+    recorded_warnings = [str(w) for w in recwarn]
+    assert recorded_warnings == []
 
 
 def test_lasso_lars_ic():
diff --git a/sklearn/metrics/tests/test_pairwise.py b/sklearn/metrics/tests/test_pairwise.py
--- a/sklearn/metrics/tests/test_pairwise.py
+++ b/sklearn/metrics/tests/test_pairwise.py
@@ -48,6 +48,7 @@
 from sklearn.metrics.pairwise import paired_distances
 from sklearn.metrics.pairwise import paired_euclidean_distances
 from sklearn.metrics.pairwise import paired_manhattan_distances
+from sklearn.metrics.pairwise import _euclidean_distances_upcast
 from sklearn.preprocessing import normalize
 from sklearn.exceptions import DataConversionWarning
 
@@ -692,6 +693,52 @@ def test_euclidean_distances_sym(dtype, x_array_constr):
     assert distances.dtype == dtype
 
 
+@pytest.mark.parametrize("batch_size", [None, 5, 7, 101])
+@pytest.mark.parametrize("x_array_constr", [np.array, csr_matrix],
+                         ids=["dense", "sparse"])
+@pytest.mark.parametrize("y_array_constr", [np.array, csr_matrix],
+                         ids=["dense", "sparse"])
+def test_euclidean_distances_upcast(batch_size, x_array_constr,
+                                    y_array_constr):
+    # check batches handling when Y != X (#13910)
+    rng = np.random.RandomState(0)
+    X = rng.random_sample((100, 10)).astype(np.float32)
+    X[X < 0.8] = 0
+    Y = rng.random_sample((10, 10)).astype(np.float32)
+    Y[Y < 0.8] = 0
+
+    expected = cdist(X, Y)
+
+    X = x_array_constr(X)
+    Y = y_array_constr(Y)
+    distances = _euclidean_distances_upcast(X, Y=Y, batch_size=batch_size)
+    distances = np.sqrt(np.maximum(distances, 0))
+
+    # the default rtol=1e-7 is too close to the float32 precision
+    # and fails due too rounding errors.
+    assert_allclose(distances, expected, rtol=1e-6)
+
+
+@pytest.mark.parametrize("batch_size", [None, 5, 7, 101])
+@pytest.mark.parametrize("x_array_constr", [np.array, csr_matrix],
+                         ids=["dense", "sparse"])
+def test_euclidean_distances_upcast_sym(batch_size, x_array_constr):
+    # check batches handling when X is Y (#13910)
+    rng = np.random.RandomState(0)
+    X = rng.random_sample((100, 10)).astype(np.float32)
+    X[X < 0.8] = 0
+
+    expected = squareform(pdist(X))
+
+    X = x_array_constr(X)
+    distances = _euclidean_distances_upcast(X, Y=X, batch_size=batch_size)
+    distances = np.sqrt(np.maximum(distances, 0))
+
+    # the default rtol=1e-7 is too close to the float32 precision
+    # and fails due too rounding errors.
+    assert_allclose(distances, expected, rtol=1e-6)
+
+
 @pytest.mark.parametrize(
     "dtype, eps, rtol",
     [(np.float32, 1e-4, 1e-5),
diff --git a/sklearn/preprocessing/tests/test_encoders.py b/sklearn/preprocessing/tests/test_encoders.py
--- a/sklearn/preprocessing/tests/test_encoders.py
+++ b/sklearn/preprocessing/tests/test_encoders.py
@@ -590,6 +590,21 @@ def test_one_hot_encoder_feature_names_unicode():
     assert_array_equal(['nüëçme_c‚ù§t1', 'nüëçme_dat2'], feature_names)
 
 
+@pytest.mark.parametrize("drop, expected_names",
+                         [('first', ['x0_c', 'x2_b']),
+                          (['c', 2, 'b'], ['x0_b', 'x2_a'])],
+                         ids=['first', 'manual'])
+def test_one_hot_encoder_feature_names_drop(drop, expected_names):
+    X = [['c', 2, 'a'],
+         ['b', 2, 'b']]
+
+    ohe = OneHotEncoder(drop=drop)
+    ohe.fit(X)
+    feature_names = ohe.get_feature_names()
+    assert isinstance(feature_names, np.ndarray)
+    assert_array_equal(expected_names, feature_names)
+
+
 @pytest.mark.parametrize("X", [np.array([[1, np.nan]]).T,
                                np.array([['a', np.nan]], dtype=object).T],
                          ids=['numeric', 'object'])
diff --git a/sklearn/utils/tests/test_sparsefuncs.py b/sklearn/utils/tests/test_sparsefuncs.py
--- a/sklearn/utils/tests/test_sparsefuncs.py
+++ b/sklearn/utils/tests/test_sparsefuncs.py
@@ -393,14 +393,18 @@ def test_inplace_swap_column():
     [(0, np.min, np.max, False),
      (np.nan, np.nanmin, np.nanmax, True)]
 )
+@pytest.mark.parametrize("large_indices", [True, False])
 def test_min_max(dtype, axis, sparse_format, missing_values, min_func,
-                 max_func, ignore_nan):
+                 max_func, ignore_nan, large_indices):
     X = np.array([[0, 3, 0],
                   [2, -1, missing_values],
                   [0, 0, 0],
                   [9, missing_values, 7],
                   [4, 0, 5]], dtype=dtype)
     X_sparse = sparse_format(X)
+    if large_indices:
+        X_sparse.indices = X_sparse.indices.astype('int64')
+        X_sparse.indptr = X_sparse.indptr.astype('int64')
 
     mins_sparse, maxs_sparse = min_max_axis(X_sparse, axis=axis,
                                             ignore_nan=ignore_nan)