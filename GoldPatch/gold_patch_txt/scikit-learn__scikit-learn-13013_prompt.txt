# 修复代码生成提示词（实例ID：scikit-learn__scikit-learn-13013）
## 代码仓库
scikit-learn/scikit-learn

## 原始问题描述
Make use of check_is_fitted instead of manual checks
#### Description
In some places, a manual check is performed to check whether an estimator has been fitted, instead of using the `check_is_fitted` method. Due to this, the NotFittedError messages are often inconsistent.

Some examples include:
https://github.com/scikit-learn/scikit-learn/blob/486f8fc5438d4625ec05d22bb24ca5afb3c396fd/sklearn/linear_model/base.py#L253-L255
https://github.com/scikit-learn/scikit-learn/blob/486f8fc5438d4625ec05d22bb24ca5afb3c396fd/sklearn/linear_model/logistic.py#L1645-L1646

#### Steps/Code to Reproduce
Look at the code in the examples above.

#### Expected Results
Code should be using the `check_is_fitted` method from the `utils.validation` submodule. 

#### Actual Results
This check is re-implemented in various places. Error messages are not consistent.

#### Versions
n/a

#### TODO
I am happy to submit a PR to fix this. Planning to identify the places where the method is re-implemented using the search functionality on github. Please let me know if there is more clever way of doing this.



## 参考黄金补丁（正确的修复方案）
diff --git a/sklearn/decomposition/tests/test_online_lda.py b/sklearn/decomposition/tests/test_online_lda.py
--- a/sklearn/decomposition/tests/test_online_lda.py
+++ b/sklearn/decomposition/tests/test_online_lda.py
@@ -180,12 +180,12 @@ def test_lda_negative_input():
 
 
 def test_lda_no_component_error():
-    # test `transform` and `perplexity` before `fit`
+    # test `perplexity` before `fit`
     rng = np.random.RandomState(0)
     X = rng.randint(4, size=(20, 10))
     lda = LatentDirichletAllocation()
-    regex = r"^no 'components_' attribute"
-    assert_raises_regexp(NotFittedError, regex, lda.transform, X)
+    regex = ("This LatentDirichletAllocation instance is not fitted yet. "
+             "Call 'fit' with appropriate arguments before using this method.")
     assert_raises_regexp(NotFittedError, regex, lda.perplexity, X)
 
 
diff --git a/sklearn/ensemble/tests/test_forest.py b/sklearn/ensemble/tests/test_forest.py
--- a/sklearn/ensemble/tests/test_forest.py
+++ b/sklearn/ensemble/tests/test_forest.py
@@ -41,6 +41,8 @@
 from sklearn.utils.testing import ignore_warnings
 from sklearn.utils.testing import skip_if_no_parallel
 
+from sklearn.exceptions import NotFittedError
+
 from sklearn import datasets
 from sklearn.decomposition import TruncatedSVD
 from sklearn.datasets import make_classification
@@ -370,14 +372,12 @@ def mdi_importance(X_m, X, y):
     assert_less(np.abs(true_importances - importances).mean(), 0.01)
 
 
-def check_unfitted_feature_importances(name):
-    assert_raises(ValueError, getattr, FOREST_ESTIMATORS[name](random_state=0),
-                  "feature_importances_")
-
-
 @pytest.mark.parametrize('name', FOREST_ESTIMATORS)
 def test_unfitted_feature_importances(name):
-    check_unfitted_feature_importances(name)
+    err_msg = ("This {} instance is not fitted yet. Call 'fit' with "
+               "appropriate arguments before using this method.".format(name))
+    with pytest.raises(NotFittedError, match=err_msg):
+        getattr(FOREST_ESTIMATORS[name](), 'feature_importances_')
 
 
 def check_oob_score(name, X, y, n_estimators=20):
diff --git a/sklearn/utils/tests/test_estimator_checks.py b/sklearn/utils/tests/test_estimator_checks.py
--- a/sklearn/utils/tests/test_estimator_checks.py
+++ b/sklearn/utils/tests/test_estimator_checks.py
@@ -20,6 +20,7 @@
 from sklearn.utils.estimator_checks import check_estimators_unfitted
 from sklearn.utils.estimator_checks import check_fit_score_takes_y
 from sklearn.utils.estimator_checks import check_no_attributes_set_in_init
+from sklearn.utils.validation import check_is_fitted
 from sklearn.utils.estimator_checks import check_outlier_corruption
 from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier
 from sklearn.linear_model import LinearRegression, SGDClassifier
@@ -167,8 +168,7 @@ def fit(self, X, y):
         return self
 
     def predict(self, X):
-        if not hasattr(self, 'coef_'):
-            raise CorrectNotFittedError("estimator is not fitted yet")
+        check_is_fitted(self, 'coef_')
         X = check_array(X)
         return np.ones(X.shape[0])
 
@@ -434,7 +434,7 @@ def test_check_estimator_clones():
 def test_check_estimators_unfitted():
     # check that a ValueError/AttributeError is raised when calling predict
     # on an unfitted estimator
-    msg = "AttributeError or ValueError not raised by predict"
+    msg = "NotFittedError not raised by predict"
     assert_raises_regex(AssertionError, msg, check_estimators_unfitted,
                         "estimator", NoSparseClassifier())