# 修复代码生成提示词（实例ID：scikit-learn__scikit-learn-12784）
## 代码仓库
scikit-learn/scikit-learn

## 原始问题描述
KNeighborsRegressor gives different results for different n_jobs values
<!--
If your issue is a usage question, submit it here instead:
- StackOverflow with the scikit-learn tag: https://stackoverflow.com/questions/tagged/scikit-learn
- Mailing List: https://mail.python.org/mailman/listinfo/scikit-learn
For more information, see User Questions: http://scikit-learn.org/stable/support.html#user-questions
-->

<!-- Instructions For Filing a Bug: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#filing-bugs -->

#### Description
<!-- Example: Joblib Error thrown when calling fit on LatentDirichletAllocation with evaluate_every > 0-->
When using 'seuclidean' distance metric, the algorithm produces different predictions for different values of the n_jobs parameter if no V is passed as additional metric_params. This implies that if configured with n_jobs=-1 two different machines show different results depending on the number of cores. The same happens for 'mahalanobis' distance metric if no V and VI are passed as metric_params.

#### Steps/Code to Reproduce
<!--
Example:
```python
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

docs = ["Help I have a bug" for i in range(1000)]

vectorizer = CountVectorizer(input=docs, analyzer='word')
lda_features = vectorizer.fit_transform(docs)

lda_model = LatentDirichletAllocation(
    n_topics=10,
    learning_method='online',
    evaluate_every=10,
    n_jobs=4,
)
model = lda_model.fit(lda_features)
```
If the code is too long, feel free to put it in a public gist and link
it in the issue: https://gist.github.com
-->
```python
# Import required packages
import numpy as np
import pandas as pd
from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsRegressor

# Prepare the dataset
dataset = load_boston()
target = dataset.target
data = pd.DataFrame(dataset.data, columns=dataset.feature_names)

# Split the dataset
np.random.seed(42)
X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2)

# Create a regressor with seuclidean distance and passing V as additional argument
model_n_jobs_1 = KNeighborsRegressor(n_jobs=1, algorithm='brute', metric='seuclidean')
model_n_jobs_1.fit(X_train, y_train)
np.sum(model_n_jobs_1.predict(X_test)) # --> 2127.99999

# Create a regressor with seuclidean distance and passing V as additional argument
model_n_jobs_3 = KNeighborsRegressor(n_jobs=3, algorithm='brute', metric='seuclidean')
model_n_jobs_3.fit(X_train, y_train)
np.sum(model_n_jobs_3.predict(X_test)) # --> 2129.38

# Create a regressor with seuclidean distance and passing V as additional argument
model_n_jobs_all = KNeighborsRegressor(n_jobs=-1, algorithm='brute', metric='seuclidean')
model_n_jobs_all.fit(X_train, y_train)
np.sum(model_n_jobs_all.predict(X_test)) # --> 2125.29999
```

#### Expected Results
<!-- Example: No error is thrown. Please paste or describe the expected results.-->
The prediction should be always the same and not depend on the value passed to the n_jobs parameter.

#### Actual Results
<!-- Please paste or specifically describe the actual output or traceback. -->
The prediction value changes depending on the value passed to n_jobs which, in case of n_jobs=-1, makes the prediction depend on the number of cores of the machine running the code.

#### Versions
<!--
Please run the following snippet and paste the output below.
For scikit-learn >= 0.20:
import sklearn; sklearn.show_versions()
For scikit-learn < 0.20:
import platform; print(platform.platform())
import sys; print("Python", sys.version)
import numpy; print("NumPy", numpy.__version__)
import scipy; print("SciPy", scipy.__version__)
import sklearn; print("Scikit-Learn", sklearn.__version__)
-->
System
------
    python: 3.6.6 (default, Jun 28 2018, 04:42:43)  [GCC 5.4.0 20160609]
    executable: /home/mcorella/.local/share/virtualenvs/outlier_detection-8L4UL10d/bin/python3.6
    machine: Linux-4.15.0-39-generic-x86_64-with-Ubuntu-16.04-xenial

BLAS
----
    macros: NO_ATLAS_INFO=1, HAVE_CBLAS=None
    lib_dirs: /usr/lib
    cblas_libs: cblas

Python deps
-----------
    pip: 18.1
    setuptools: 40.5.0
    sklearn: 0.20.0
    numpy: 1.15.4
    scipy: 1.1.0
    Cython: None
    pandas: 0.23.4

<!-- Thanks for contributing! -->

utils.validation.check_array throws bad TypeError pandas series is passed in
#### Description
validation.check_array throws bad TypeError pandas series is passed in. It cropped up when using the RandomizedSearchCV class.  Caused when line 480 is executed

480 - if hasattr(array, "dtypes") and len(array.dtypes):

#### Steps/Code to Reproduce

validation.check_array(y, ensure_2d=False, dtype=None) where y is a pandas series

#### Expected Results
No error (I'm not familiar with this code so not sure on the details)

#### Actual Results
TypeError: object of type 'DTYPE NAME OF THE SERIES' has no len()

#### Versions
0.20.1


## 参考黄金补丁（正确的修复方案）
diff --git a/sklearn/compose/tests/test_column_transformer.py b/sklearn/compose/tests/test_column_transformer.py
--- a/sklearn/compose/tests/test_column_transformer.py
+++ b/sklearn/compose/tests/test_column_transformer.py
@@ -542,6 +542,20 @@ def test_make_column_transformer():
                                 ('first', 'drop'))
 
 
+def test_make_column_transformer_pandas():
+    pd = pytest.importorskip('pandas')
+    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T
+    X_df = pd.DataFrame(X_array, columns=['first', 'second'])
+    norm = Normalizer()
+    # XXX remove in v0.22
+    with pytest.warns(DeprecationWarning,
+                      match='`make_column_transformer` now expects'):
+        ct1 = make_column_transformer((X_df.columns, norm))
+    ct2 = make_column_transformer((norm, X_df.columns))
+    assert_almost_equal(ct1.fit_transform(X_df),
+                        ct2.fit_transform(X_df))
+
+
 def test_make_column_transformer_kwargs():
     scaler = StandardScaler()
     norm = Normalizer()
diff --git a/sklearn/linear_model/tests/test_logistic.py b/sklearn/linear_model/tests/test_logistic.py
--- a/sklearn/linear_model/tests/test_logistic.py
+++ b/sklearn/linear_model/tests/test_logistic.py
@@ -1150,14 +1150,31 @@ def test_logreg_l1_sparse_data():
 
 @pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22
 @pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22
-def test_logreg_cv_penalty():
-    # Test that the correct penalty is passed to the final fit.
-    X, y = make_classification(n_samples=50, n_features=20, random_state=0)
-    lr_cv = LogisticRegressionCV(penalty="l1", Cs=[1.0], solver='saga')
+@pytest.mark.parametrize("random_seed", [42])
+@pytest.mark.parametrize("penalty", ["l1", "l2"])
+def test_logistic_regression_cv_refit(random_seed, penalty):
+    # Test that when refit=True, logistic regression cv with the saga solver
+    # converges to the same solution as logistic regression with a fixed
+    # regularization parameter.
+    # Internally the LogisticRegressionCV model uses a warm start to refit on
+    # the full data model with the optimal C found by CV. As the penalized
+    # logistic regression loss is convex, we should still recover exactly
+    # the same solution as long as the stopping criterion is strict enough (and
+    # that there are no exactly duplicated features when penalty='l1').
+    X, y = make_classification(n_samples=50, n_features=20,
+                               random_state=random_seed)
+    common_params = dict(
+        solver='saga',
+        penalty=penalty,
+        random_state=random_seed,
+        max_iter=10000,
+        tol=1e-12,
+    )
+    lr_cv = LogisticRegressionCV(Cs=[1.0], refit=True, **common_params)
     lr_cv.fit(X, y)
-    lr = LogisticRegression(penalty="l1", C=1.0, solver='saga')
+    lr = LogisticRegression(C=1.0, **common_params)
     lr.fit(X, y)
-    assert_equal(np.count_nonzero(lr_cv.coef_), np.count_nonzero(lr.coef_))
+    assert_array_almost_equal(lr_cv.coef_, lr.coef_)
 
 
 def test_logreg_predict_proba_multinomial():
diff --git a/sklearn/metrics/tests/test_pairwise.py b/sklearn/metrics/tests/test_pairwise.py
--- a/sklearn/metrics/tests/test_pairwise.py
+++ b/sklearn/metrics/tests/test_pairwise.py
@@ -5,9 +5,12 @@
 
 from scipy.sparse import dok_matrix, csr_matrix, issparse
 from scipy.spatial.distance import cosine, cityblock, minkowski, wminkowski
+from scipy.spatial.distance import cdist, pdist, squareform
 
 import pytest
 
+from sklearn import config_context
+
 from sklearn.utils.testing import assert_greater
 from sklearn.utils.testing import assert_array_almost_equal
 from sklearn.utils.testing import assert_allclose
@@ -893,3 +896,39 @@ def test_check_preserve_type():
                                                    XB.astype(np.float))
     assert_equal(XA_checked.dtype, np.float)
     assert_equal(XB_checked.dtype, np.float)
+
+
+@pytest.mark.parametrize("n_jobs", [1, 2])
+@pytest.mark.parametrize("metric", ["seuclidean", "mahalanobis"])
+@pytest.mark.parametrize("dist_function",
+                         [pairwise_distances, pairwise_distances_chunked])
+@pytest.mark.parametrize("y_is_x", [True, False], ids=["Y is X", "Y is not X"])
+def test_pairwise_distances_data_derived_params(n_jobs, metric, dist_function,
+                                                y_is_x):
+    # check that pairwise_distances give the same result in sequential and
+    # parallel, when metric has data-derived parameters.
+    with config_context(working_memory=0.1):  # to have more than 1 chunk
+        rng = np.random.RandomState(0)
+        X = rng.random_sample((1000, 10))
+
+        if y_is_x:
+            Y = X
+            expected_dist_default_params = squareform(pdist(X, metric=metric))
+            if metric == "seuclidean":
+                params = {'V': np.var(X, axis=0, ddof=1)}
+            else:
+                params = {'VI': np.linalg.inv(np.cov(X.T)).T}
+        else:
+            Y = rng.random_sample((1000, 10))
+            expected_dist_default_params = cdist(X, Y, metric=metric)
+            if metric == "seuclidean":
+                params = {'V': np.var(np.vstack([X, Y]), axis=0, ddof=1)}
+            else:
+                params = {'VI': np.linalg.inv(np.cov(np.vstack([X, Y]).T)).T}
+
+        expected_dist_explicit_params = cdist(X, Y, metric=metric, **params)
+        dist = np.vstack(tuple(dist_function(X, Y,
+                                             metric=metric, n_jobs=n_jobs)))
+
+        assert_allclose(dist, expected_dist_explicit_params)
+        assert_allclose(dist, expected_dist_default_params)
diff --git a/sklearn/model_selection/tests/test_split.py b/sklearn/model_selection/tests/test_split.py
--- a/sklearn/model_selection/tests/test_split.py
+++ b/sklearn/model_selection/tests/test_split.py
@@ -458,13 +458,13 @@ def test_shuffle_kfold():
 
 
 def test_shuffle_kfold_stratifiedkfold_reproducibility():
-    # Check that when the shuffle is True multiple split calls produce the
-    # same split when random_state is set
     X = np.ones(15)  # Divisible by 3
     y = [0] * 7 + [1] * 8
     X2 = np.ones(16)  # Not divisible by 3
     y2 = [0] * 8 + [1] * 8
 
+    # Check that when the shuffle is True, multiple split calls produce the
+    # same split when random_state is int
     kf = KFold(3, shuffle=True, random_state=0)
     skf = StratifiedKFold(3, shuffle=True, random_state=0)
 
@@ -472,8 +472,12 @@ def test_shuffle_kfold_stratifiedkfold_reproducibility():
         np.testing.assert_equal(list(cv.split(X, y)), list(cv.split(X, y)))
         np.testing.assert_equal(list(cv.split(X2, y2)), list(cv.split(X2, y2)))
 
-    kf = KFold(3, shuffle=True)
-    skf = StratifiedKFold(3, shuffle=True)
+    # Check that when the shuffle is True, multiple split calls often
+    # (not always) produce different splits when random_state is
+    # RandomState instance or None
+    kf = KFold(3, shuffle=True, random_state=np.random.RandomState(0))
+    skf = StratifiedKFold(3, shuffle=True,
+                          random_state=np.random.RandomState(0))
 
     for cv in (kf, skf):
         for data in zip((X, X2), (y, y2)):
diff --git a/sklearn/neighbors/tests/test_dist_metrics.py b/sklearn/neighbors/tests/test_dist_metrics.py
--- a/sklearn/neighbors/tests/test_dist_metrics.py
+++ b/sklearn/neighbors/tests/test_dist_metrics.py
@@ -6,6 +6,8 @@
 
 import pytest
 
+from distutils.version import LooseVersion
+from scipy import __version__ as scipy_version
 from scipy.spatial.distance import cdist
 from sklearn.neighbors.dist_metrics import DistanceMetric
 from sklearn.neighbors import BallTree
@@ -101,6 +103,11 @@ def check_pdist(metric, kwargs, D_true):
 def check_pdist_bool(metric, D_true):
     dm = DistanceMetric.get_metric(metric)
     D12 = dm.pairwise(X1_bool)
+    # Based on https://github.com/scipy/scipy/pull/7373
+    # When comparing two all-zero vectors, scipy>=1.2.0 jaccard metric
+    # was changed to return 0, instead of nan.
+    if metric == 'jaccard' and LooseVersion(scipy_version) < '1.2.0':
+        D_true[np.isnan(D_true)] = 0
     assert_array_almost_equal(D12, D_true)
 
 
diff --git a/sklearn/utils/tests/test_validation.py b/sklearn/utils/tests/test_validation.py
--- a/sklearn/utils/tests/test_validation.py
+++ b/sklearn/utils/tests/test_validation.py
@@ -700,6 +700,11 @@ def test_check_array_series():
                       warn_on_dtype=True)
     assert_array_equal(res, np.array([1, 2, 3]))
 
+    # with categorical dtype (not a numpy dtype) (GH12699)
+    s = pd.Series(['a', 'b', 'c']).astype('category')
+    res = check_array(s, dtype=None, ensure_2d=False)
+    assert_array_equal(res, np.array(['a', 'b', 'c'], dtype=object))
+
 
 def test_check_dataframe_warns_on_dtype():
     # Check that warn_on_dtype also works for DataFrames.