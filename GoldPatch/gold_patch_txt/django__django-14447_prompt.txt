# 修复代码生成提示词（实例ID：django__django-14447）
## 代码仓库
django/django

## 原始问题描述
Database cache counts the DB size twice at a performance penalty
Description
	
We have a lot of entries in the DB cache, and I've noticed that the following query shows up in my slow query log kind of a lot (Postgresql is slow at counting things):
SELECT COUNT(*) FROM cache_table;
This query is being run by the DB cache twice for every cache update in order to determine if culling is needed. First, in the cache setting code, it runs:
			cursor.execute("SELECT COUNT(*) FROM %s" % table)
			num = cursor.fetchone()[0]
			now = timezone.now()
			now = now.replace(microsecond=0)
			if num > self._max_entries:
				self._cull(db, cursor, now)
(​https://github.com/django/django/blob/d06c5b358149c02a62da8a5469264d05f29ac659/django/core/cache/backends/db.py#L120-L131)
Then in self._cull (the last line above) it runs:
			cursor.execute("DELETE FROM %s WHERE expires < %%s" % table,
						 [connection.ops.adapt_datetimefield_value(now)])
			cursor.execute("SELECT COUNT(*) FROM %s" % table)
			num = cursor.fetchone()[0]
			if num > self._max_entries:
				# Do culling routine here...
(​https://github.com/django/django/blob/d06c5b358149c02a62da8a5469264d05f29ac659/django/core/cache/backends/db.py#L254-L260)
The idea is that if the MAX_ENTRIES setting is exceeded, it'll cull the DB cache down by some percentage so it doesn't grow forever. 
I think that's fine, but given that the SELECT COUNT(*) query is slow, I wonder two things:
Would a refactor to remove the second query be a good idea? If you pass the count from the first query into the _cull method, you can then do:
		def _cull(self, db, cursor, now, count):
			...
			cursor.execute("DELETE FROM %s WHERE expires < %%s" % table,
						 [connection.ops.adapt_datetimefield_value(now)])
			deleted_count = cursor.rowcount
			num = count - deleted_count
			if num > self._max_entries:
				# Do culling routine here...
That seems like a simple win.
Is it reasonable to not run the culling code *every* time that we set a value? Like, could we run it every tenth time or every 100th time or something? 
If this is a good idea, does anybody have a proposal for how to count this? I'd be happy just doing it on a mod of the current millisecond, but there's probably a better way (randint?). 
Would a setting be a good idea here? We already have MAX_ENTRIES and CULL_FREQUENCY. CULL_FREQUENCY is "the fraction of entries that are culled when MAX_ENTRIES is reached." That sounds more like it should have been named CULL_RATIO (regrets!), but maybe a new setting for this could be called "CULL_EVERY_X"? 
I think the first change is a no-brainer, but both changes seem like wins to me. Happy to implement either or both of these, but wanted buy-in first.


## 参考黄金补丁（正确的修复方案）
diff --git a/tests/cache/tests.py b/tests/cache/tests.py
--- a/tests/cache/tests.py
+++ b/tests/cache/tests.py
@@ -40,6 +40,7 @@
     ignore_warnings, override_settings,
 )
 from django.test.signals import setting_changed
+from django.test.utils import CaptureQueriesContext
 from django.utils import timezone, translation
 from django.utils.cache import (
     get_cache_key, learn_cache_key, patch_cache_control, patch_vary_headers,
@@ -1117,6 +1118,18 @@ def test_delete_many_num_queries(self):
         with self.assertNumQueries(1):
             cache.delete_many(['a', 'b', 'c'])
 
+    def test_cull_count_queries(self):
+        old_max_entries = cache._max_entries
+        # Force _cull to delete on first cached record.
+        cache._max_entries = -1
+        with CaptureQueriesContext(connection) as captured_queries:
+            try:
+                cache.set('force_cull', 'value', 1000)
+            finally:
+                cache._max_entries = old_max_entries
+        num_count_queries = sum('COUNT' in query['sql'] for query in captured_queries)
+        self.assertEqual(num_count_queries, 1)
+
     def test_delete_cursor_rowcount(self):
         """
         The rowcount attribute should not be checked on a closed cursor.